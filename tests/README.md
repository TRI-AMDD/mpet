# Regression Testing

When adding new features or making changes to the code, it's helpful to run a suite of tests to make sure various things are behaving as expected. This should not be necessary for users who are not changing the code at all, although it could still be nice to verify that the outputs users are seeing match those the developers expect for a few specific cases.

## Prerequisites

The testing suite has additional dependencies, which can be installed from MPET's root directory with `pip install .[test]`. These additional dependencies are: `pytest`, `coverage`, `coveralls`, and `flake8`.

## Running

To run the tests, execute `PYTHONPATH=. ./bin/mpettest.py` from the repository root. The `-h` flag shows which arguments are accepted.  This will run a number of
simulations and test the results.

To compare the output manually you can use pytest:
```bash
  pytest --baseDir=tests/ref_outputs/ --modDir=tests/test_outputs/20201208_154137/ tests/compare_tests.py
```
or you can compare specific tests:
```bash
  pytest --baseDir=tests/ref_outputs/ --modDir=tests/test_outputs/20201208_154137/ tests/compare_tests.py --tests test001 test002 --skip-analytic
```
or you can compare timings:
```bash
  pytest --baseDir=tests/ref_outputs/ --modDir=tests/test_outputs/20201208_154137/ tests/compare_timings.py --tests test001 test002 --skip-analytic
```

You can also compare different output folders, or against the reference solution.

Note that tests may "fail" even when things are okay, resulting from small numerical differences. If tests fail, it is helpful to look at the comparison plots generated by default within `mpet/tests/test_outputs/[time-stamped-directory]/plots` to see if the differences seem significant.
There is also the flag --tolerance which can be used in combination with pytest
it is both tested as absolute and relative tolerance.

```bash
  pytest --tolerance=0.01 --baseDir=tests/ref_outputs/ --modDir=tests/test_outputs/20201208_154137/ tests/compare_tests.py
```


# List of tests

 - benchmark_LIONSIMBA: isothermal comparison with the problem studied in Torchio et al., 2016.
 - benhmark_LIONSIMBA_nonisothermal: constant temperature comparison (323K without heat generation) using the nonisothermal model implemented in LIONSIMBA
 - test001: LFP ACR C3
 - test002: LFP CHR cylinder
 - test003: LFP CHR sphere
 - test004: LFP CHR sphere with noise
 - test005: LFP homog
 - test006: LFP homog with Vmin
 - test007: LFP homog with Vmax
 - test008: LFP homog_sdn
 - test009: Graphite-2param homog
 - test010: Graphite-2param CHR cylinder
 - test011: Graphite-2param CHR sphere
 - test012: Solid solution, diffn sphere, homog, LiC6_coke_ss2, LiMn2O4_ss2, BV_mod01, BV_mod02, cathode + anode
 - test013: Solid solution, diffn cylinder, homog, testIS_ss, LiMn2O4_ss2, Marcus, BV_raw, cathode + separator + anode
 - test014: LFP homog with CCsegments, MHC, Rser
 - test015: testRS homog with CVsegments, bulkCond, partCond
 - test016: test CC continuation
 - test017: test CV continuation
 - test018: Like test014, LFP homog with CCsegments, BV, Rfilm, Rfilm_foil
 - test019: SM electrolyte with LFP homog, sep + cathode, significant elyte polarization
 - test020: hdf5 file output
 - test021: hdf5Fast file output and restarting hdf5 simulations
 - test022: Test of specified_psd_c option, LFP homog
 - test023: CIET for LFP
 - test028: CCCVCPcycling for full cell with LIONSIMBA test file
 - test029: CCCVCPcycling for half cell with LIONSIMBA test file
